{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "HwnqO8VdtjYV"
      },
      "outputs": [],
      "source": [
        "import bs4 as bs\n",
        "import requests\n",
        "import yfinance as yf\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Nl9euQWRCaRE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stock Price Data"
      ],
      "metadata": {
        "id": "kTPnFkJYFJp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
        "soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
        "table = soup.find('table', {'class': 'wikitable sortable'})\n",
        "\n",
        "tickers = []\n",
        "for row in table.findAll('tr')[1:]:\n",
        "    ticker = row.findAll('td')[0].text\n",
        "    tickers.append(ticker)\n",
        "tickers = [s.replace('\\n', '') for s in tickers]"
      ],
      "metadata": {
        "id": "nTNDsHzzFKmC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_single_stock(stock_data):\n",
        "  full = pd.DataFrame()\n",
        "  full['ma5'] = stock_data.rolling(5).mean()\n",
        "  full['ma10'] = stock_data.rolling(10).mean()\n",
        "  full['ma20'] = stock_data.rolling(20).mean()\n",
        "  full['ma30'] = stock_data.rolling(30).mean()\n",
        "  full['close'] = stock_data\n",
        "  full = full.dropna(axis = 0)\n",
        "  price_max = np.max(full['close'])\n",
        "  return full / price_max"
      ],
      "metadata": {
        "id": "eHUSmbExQGo8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_all_stocks(tickers, start, end, steps):\n",
        "  data = yf.download(tickers, start = start, end = end)\n",
        "  data = data['Close']\n",
        "  data = data.dropna(axis = 1)\n",
        "  for i in range(len(data.columns)):\n",
        "    single_EOD = process_single_stock(data.iloc[:, i])\n",
        "    if (i == 0):\n",
        "      eod_data = np.zeros([len(data.columns), single_EOD.shape[0], single_EOD.shape[1]])\n",
        "      ground_truth = np.zeros([len(data.columns), single_EOD.shape[0]])\n",
        "      close_prices = np.zeros([len(data.columns), single_EOD.shape[0]])\n",
        "    eod_data[i, :, :] = single_EOD\n",
        "    close_prices[i, :] = single_EOD.iloc[:, -1]\n",
        "    for row in range(single_EOD.shape[0]):\n",
        "      if (row > steps - 1):\n",
        "        ground_truth[i, row] = (single_EOD.iloc[row][-1] - single_EOD.iloc[row - steps][-1]) / single_EOD.iloc[row - steps][-1]\n",
        "  return data.columns, eod_data, close_prices, ground_truth"
      ],
      "metadata": {
        "id": "xobpJM6wTUDT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime(2014, 1, 1)\n",
        "end = datetime.datetime(2024, 1, 1)\n",
        "steps = 1"
      ],
      "metadata": {
        "id": "WUtbNxmJFWks"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stocks, eod_data, price_data, gt_data = process_all_stocks(tickers, start, end, steps)"
      ],
      "metadata": {
        "id": "CfQ72GDPccR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of Stocks: {}\".format(len(stocks)))\n",
        "print(\"Shape of EOD data: {}\".format(eod_data.shape))\n",
        "print(\"Shape of Price data: {}\".format(price_data.shape))\n",
        "print(\"Shape of Ground Truth data: {}\".format(gt_data.shape))"
      ],
      "metadata": {
        "id": "ndOgDiJYNiLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stock Relation Data"
      ],
      "metadata": {
        "id": "Kqk2L6GrpVUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df):\n",
        "  returns = df.pct_change()\n",
        "  returns = returns.drop(returns.index[0])\n",
        "  returns = returns.dropna(axis = 1)\n",
        "  return returns"
      ],
      "metadata": {
        "id": "auEzlWBtsmG9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_simple_correlation(df, method):\n",
        "  returns = preprocess(df)\n",
        "  return returns.corr(method = method)"
      ],
      "metadata": {
        "id": "2c_JWLzJtK7R"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_granger(df, maxlag):\n",
        "  returns = preprocess(df)\n",
        "  variables = returns.columns\n",
        "  granger = pd.DataFrame(np.zeros((len(variables), len(variables))), columns = variables, index = variables)\n",
        "  for row in granger.index:\n",
        "    for col in granger.columns:\n",
        "      test_result = grangercausalitytests(returns[[row, col]], maxlag, verbose = None)\n",
        "      p_vals = [round(test_result[i + 1][0][\"ssr_chi2test\"][1], 3) for i in range(maxlag)]\n",
        "      min_p_val = np.min(p_vals)\n",
        "      granger.loc[row, col] = min_p_val\n",
        "  return granger"
      ],
      "metadata": {
        "id": "RzHNHxoAMFiN"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relation_start = datetime.datetime(2023, 1, 1)\n",
        "relation_data = yf.download(stocks.tolist(), start = relation_start, end = end)\n",
        "relation_data = relation_data[\"Close\"]"
      ],
      "metadata": {
        "id": "Sw362SnHpW7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pearson = compute_simple_correlation(relation_data, \"pearson\")\n",
        "pearson[pearson == 1] = 0\n",
        "pearson[abs(pearson) >= 0.5] = 1\n",
        "pearson[abs(pearson) < 0.5] = 0\n",
        "pearson = np.expand_dims(np.array(pearson), axis = 2)"
      ],
      "metadata": {
        "id": "ZbEJ58x8tQIU"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spearman = compute_simple_correlation(relation_data, \"spearman\")\n",
        "spearman[spearman == 1] = 0\n",
        "spearman[abs(spearman) >= 0.5] = 1\n",
        "spearman[abs(spearman) < 0.5] = 0\n",
        "spearman = np.expand_dims(np.array(spearman), axis = 2)"
      ],
      "metadata": {
        "id": "6ssYnnDnKVFL"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "granger = compute_granger(relation_data, 5)\n",
        "granger[granger == 1] = 0\n",
        "granger[granger <= 0.05] = 1\n",
        "granger[granger > 0.05] = 0\n",
        "granger = np.expand_dims(np.array(granger), axis = 2)"
      ],
      "metadata": {
        "id": "TAPZDbq8MFD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rank LSTM"
      ],
      "metadata": {
        "id": "qeZkLawVuOXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RankLSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RankLSTM, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    self.leaky_relu = nn.LeakyReLU(negative_slope = 0.2)\n",
        "\n",
        "  def forward(self, input): # input: (batch size, sequence length, input size)\n",
        "    x, _ = self.lstm(input) # x: (batch size, sequence length, hidden size)\n",
        "    seq_embed = x[:, -1, :] # seq_embed: (batch size, hidden size)\n",
        "    out = self.fc(seq_embed) # out: (batch size, output size)\n",
        "    preds = self.leaky_relu(out) # preds: (batch size, output size)\n",
        "    return preds"
      ],
      "metadata": {
        "id": "Fsd9eeETtwDM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Relational Stock Ranking"
      ],
      "metadata": {
        "id": "8S2Tnt3OE5D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphModule(nn.Module):\n",
        "  def __init__(self, rel_encoding, input_size, directed):\n",
        "    super(GraphModule, self).__init__()\n",
        "    self.relation = nn.Parameter(torch.tensor(rel_encoding, dtype = torch.float32), requires_grad = False) # self.relation: (batch size, batch size, 1)\n",
        "    self.all_one = nn.Parameter(torch.ones(len(stocks), 1, dtype = torch.float32), requires_grad = False) # self.all_one: (batch size, 1)\n",
        "    self.rel_weight = nn.Linear(rel_encoding.shape[-1], 1)\n",
        "    self.head_weight = nn.Linear(input_size, 1)\n",
        "    self.directed = directed\n",
        "    if (directed):\n",
        "      self.tail_weight = nn.Linear(input_size, 1)\n",
        "    self.leaky_relu = nn.LeakyReLU(negative_slope = 0.2)\n",
        "    self.softmax = nn.Softmax(dim = 0)\n",
        "\n",
        "  def forward(self, input): # input: (batch size, hidden size)\n",
        "    rel_weight = self.leaky_relu(self.rel_weight(self.relation)) # rel_weight: (batch size, batch size, 1)\n",
        "    rel_weight = rel_weight[:, :, -1] # rel_weight: (batch size, batch size)\n",
        "    head_weight = self.leaky_relu(self.head_weight(input)) # head_weight: (batch size, 1)\n",
        "    all_one = self.all_one\n",
        "    if (self.directed):\n",
        "      tail_weight = self.leaky_relu(self.tail_weight(input)) # tail_weight: (batch size, 1)\n",
        "      weight = (head_weight @ all_one.t() + all_one @ tail_weight.t()) + rel_weight # weight: (batch size, batch size)\n",
        "    else:\n",
        "      weight = (head_weight @ all_one.t() + all_one @ head_weight.t()) + rel_weight # weight: (batch size, batch size)\n",
        "    weight = self.softmax(weight) # weight: (batch size, batch size)\n",
        "    output = weight @ input # output: (batch size, hidden size)\n",
        "    return output"
      ],
      "metadata": {
        "id": "Jme48VHiHTLE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RSR(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, rel_encoding, directed):\n",
        "    super(RSR, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True)\n",
        "    self.graph_layer = GraphModule(rel_encoding, hidden_size, directed)\n",
        "    self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "    self.leaky_relu = nn.LeakyReLU(negative_slope = 0.2)\n",
        "\n",
        "  def forward(self, input): # input: (batch size, sequence length, input size)\n",
        "    x, _ = self.lstm(input) # x: (batch size, sequence length, hidden size)\n",
        "    seq_embed = x[:, -1, :] # seq_embed: (batch size, hidden size)\n",
        "    rel_embed = self.graph_layer(seq_embed) # rel_embed: (batch size, hidden size)\n",
        "    full_embed = torch.cat((seq_embed, rel_embed), dim = 1) # full_embed: (batch size, hidden_size * 2)\n",
        "    out = self.fc(full_embed) # out: (batch size, output size)\n",
        "    preds = self.leaky_relu(out) # preds: (batch size, output size)\n",
        "    return preds"
      ],
      "metadata": {
        "id": "E3rWH_N3E7CD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train, Tune, and Test"
      ],
      "metadata": {
        "id": "br69VtzqCldG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(eod_data, price_data, gt_data, offset, seq_len, steps):\n",
        "  eod_batch = eod_data[:, offset:offset + seq_len, :]\n",
        "  price_batch = np.expand_dims(price_data[:, offset + seq_len - 1], axis = 1)\n",
        "  gt_batch = np.expand_dims(gt_data[:, offset + seq_len + steps - 1], axis = 1)\n",
        "  return eod_batch, price_batch, gt_batch"
      ],
      "metadata": {
        "id": "ay-FRUFn-lrz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss(prediction, price_batch, gt_batch, alpha):\n",
        "  return_ratio = torch.div(torch.sub(prediction, price_batch), price_batch)\n",
        "  reg_loss = F.mse_loss(gt_batch, return_ratio)\n",
        "  all_one = torch.ones(len(stocks), 1, dtype = torch.float32).to(device)\n",
        "  pred_pw_diff = torch.sub(return_ratio @ all_one.t(), all_one @ return_ratio.t())\n",
        "  gt_pw_diff = torch.sub(all_one @ gt_batch.t(), gt_batch @ all_one.t())\n",
        "  rank_loss = torch.mean(F.relu(pred_pw_diff * gt_pw_diff))\n",
        "  loss = reg_loss + alpha * rank_loss\n",
        "  return loss, reg_loss, rank_loss, return_ratio"
      ],
      "metadata": {
        "id": "CJidqwlYAe5U"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(prediction, ground_truth):\n",
        "  performance = {}\n",
        "  performance[\"mse\"] = mean_squared_error(prediction, ground_truth)\n",
        "  mrr = 0\n",
        "  bt_top1 = 0\n",
        "  bt_top5 = 0\n",
        "  bt_top10 = 0\n",
        "\n",
        "  for i in range(prediction.shape[1]):\n",
        "    rank_gt = np.argsort(ground_truth[:, i])\n",
        "    rank_pred = np.argsort(prediction[:, i])\n",
        "    pred_top1 = rank_pred[-1]\n",
        "    pred_top5 = rank_pred[range(-1, -6, -1)]\n",
        "    pred_top10 = rank_pred[range(-1, -11, -1)]\n",
        "\n",
        "    # MRR for top 1\n",
        "    top1_pos_in_gt = 0\n",
        "    for j in range(1, prediction.shape[0] + 1):\n",
        "      rank = rank_gt[-1 * j]\n",
        "      top1_pos_in_gt += 1\n",
        "      if (rank == pred_top1):\n",
        "        break;\n",
        "    mrr += 1.0 / top1_pos_in_gt\n",
        "\n",
        "    # Back-testing for Investment Return Ratio\n",
        "    real_rr_top1 = ground_truth[pred_top1][i]\n",
        "    bt_top1 += real_rr_top1\n",
        "\n",
        "    real_rr_top5 = 0\n",
        "    for pred in pred_top5:\n",
        "      real_rr_top5 += ground_truth[pred][i]\n",
        "    real_rr_top5 /= 5\n",
        "    bt_top5 += real_rr_top5\n",
        "\n",
        "    real_rr_top10 = 0\n",
        "    for pred in pred_top10:\n",
        "      real_rr_top10 += ground_truth[pred][i]\n",
        "    real_rr_top10 /= 10\n",
        "    bt_top10 += real_rr_top10\n",
        "\n",
        "  performance[\"mrr\"] = mrr\n",
        "  performance[\"bt_top1\"] = bt_top1\n",
        "  performance[\"bt_top5\"] = bt_top5\n",
        "  performance[\"bt_top10\"] = bt_top10\n",
        "\n",
        "  return performance"
      ],
      "metadata": {
        "id": "jWQZk6NP4JTE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(start_index, end_index, eod_data, price_data, gt_data):\n",
        "  with torch.no_grad():\n",
        "    pred = np.zeros((len(stocks), end_index - start_index), dtype = float)\n",
        "    gt = np.zeros((len(stocks), end_index - start_index), dtype = float)\n",
        "    test_loss = 0\n",
        "    test_reg_loss = 0\n",
        "    test_rank_loss = 0\n",
        "    for offset in range(start_index - seq_len - steps + 1, end_index - seq_len - steps + 1):\n",
        "      eod_batch, price_batch, gt_batch = map(lambda x: torch.Tensor(x).to(device),\n",
        "                                             get_batch(eod_data, price_data, gt_data, offset, seq_len, steps))\n",
        "      prediction = model(eod_batch)\n",
        "      loss, reg_loss, rank_loss, return_ratio = get_loss(prediction, price_batch, gt_batch, alpha)\n",
        "      test_loss += loss.item()\n",
        "      test_reg_loss += reg_loss.item()\n",
        "      test_rank_loss += rank_loss.item()\n",
        "      pred[:, offset - (start_index - seq_len - steps + 1)] = return_ratio[:, 0].cpu()\n",
        "      gt[:, offset - (start_index - seq_len - steps + 1)] = gt_batch[:, 0].cpu()\n",
        "    test_loss = test_loss / (end_index - start_index)\n",
        "    test_reg_loss = test_reg_loss / (end_index - start_index)\n",
        "    test_rank_loss = test_rank_loss / (end_index - start_index)\n",
        "    performance = evaluate(pred, gt)\n",
        "  return test_loss, test_reg_loss, test_rank_loss, performance"
      ],
      "metadata": {
        "id": "d3iUZ50v2xut"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 229\n",
        "val_split = 2000\n",
        "test_split = 2240\n",
        "offsets = np.arange(0, val_split)\n",
        "input_size = 5\n",
        "output_size = 1"
      ],
      "metadata": {
        "id": "IhknPlqfu-W5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank LSTM\n",
        "\n",
        "num_epochs = 50\n",
        "seq_len = 16\n",
        "alpha = 1\n",
        "hidden_size = 16\n",
        "model = RankLSTM(input_size, hidden_size, output_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-6)"
      ],
      "metadata": {
        "id": "hBvNeThPe_0K"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RSR\n",
        "\n",
        "num_epochs = 50\n",
        "seq_len = 16\n",
        "alpha = 1\n",
        "hidden_size = 16\n",
        "model = RSR(input_size, hidden_size, output_size, spearman, False).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-6)"
      ],
      "metadata": {
        "id": "crnUcfQqvar8"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(seed)\n",
        "torch.random.manual_seed(seed)\n",
        "for epoch in range(num_epochs):\n",
        "  np.random.shuffle(offsets)\n",
        "  train_loss = 0\n",
        "  train_reg_loss = 0\n",
        "  train_rank_loss = 0\n",
        "  for i in range(val_split - seq_len - steps + 1):\n",
        "    eod_batch, price_batch, gt_batch = map(lambda x: torch.Tensor(x).to(device),\n",
        "                                           get_batch(eod_data, price_data, gt_data, offsets[i], seq_len, steps))\n",
        "    optimizer.zero_grad()\n",
        "    prediction = model(eod_batch)\n",
        "    loss, reg_loss, rank_loss, _ = get_loss(prediction, price_batch, gt_batch, alpha)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "    train_reg_loss += reg_loss.item()\n",
        "    train_rank_loss += rank_loss.item()\n",
        "\n",
        "  print(\"Epoch: {}\".format(epoch))\n",
        "\n",
        "  # Train\n",
        "  train_loss = train_loss / (val_split - seq_len - steps + 1)\n",
        "  train_reg_loss = train_reg_loss / (val_split - seq_len - steps + 1)\n",
        "  train_rank_loss = train_rank_loss/ (val_split - seq_len - steps + 1)\n",
        "  print(\"\\tTrain Loss: {}\".format(train_loss))\n",
        "  print(\"\\tTrain Reg Loss: {}\".format(train_reg_loss))\n",
        "  print(\"\\tTrain Rank Loss: {}\".format(train_rank_loss))\n",
        "\n",
        "  # Val\n",
        "  val_loss, val_reg_loss, val_rank_loss, val_perform = validate(val_split, test_split, eod_data, price_data, gt_data)\n",
        "  print(\"\\tVal Loss: {}\".format(val_loss))\n",
        "  print(\"\\tVal Reg Loss: {}\".format(val_reg_loss))\n",
        "  print(\"\\tVal Rank Loss: {}\".format(val_rank_loss))\n",
        "  print(\"\\tVal MSE: {}\".format(val_perform[\"mse\"]))\n",
        "  print(\"\\tVal MRR: {}\".format(val_perform[\"mrr\"]))\n",
        "  print(\"\\tVal Top 1: {}\".format(val_perform[\"bt_top1\"]))\n",
        "  print(\"\\tVal Top 5: {}\".format(val_perform[\"bt_top5\"]))\n",
        "  print(\"\\tVal Top 10: {}\".format(val_perform[\"bt_top10\"]))\n",
        "\n",
        "  # Test\n",
        "  test_loss, test_reg_loss, test_rank_loss, test_perform = validate(test_split, gt_data.shape[1], eod_data, price_data, gt_data)\n",
        "  print(\"\\tTest Loss: {}\".format(test_loss))\n",
        "  print(\"\\tTest Reg Loss: {}\".format(test_reg_loss))\n",
        "  print(\"\\tTest Rank Loss: {}\".format(test_rank_loss))\n",
        "  print(\"\\tTest MSE: {}\".format(test_perform[\"mse\"]))\n",
        "  print(\"\\tTest MRR: {}\".format(test_perform[\"mrr\"]))\n",
        "  print(\"\\tTest Top 1: {}\".format(test_perform[\"bt_top1\"]))\n",
        "  print(\"\\tTest Top 5: {}\".format(test_perform[\"bt_top5\"]))\n",
        "  print(\"\\tTest Top 10: {}\".format(test_perform[\"bt_top10\"]))"
      ],
      "metadata": {
        "id": "zESnEFREiJQa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}